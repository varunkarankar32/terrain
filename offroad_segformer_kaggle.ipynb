{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "907bca32",
   "metadata": {},
   "source": [
    "# üåø Off-Road Semantic Segmentation with SegFormer-B5\n",
    "## Hack for Green Bharat ‚Äî God-Level Pipeline\n",
    "\n",
    "**Architecture:** SegFormer-B5 (Mix Transformer Encoder + All-MLP Decoder)  \n",
    "**Backbone:** `nvidia/segformer-b5-finetuned-ade-640-640` (pretrained on ADE20K, 150 classes ‚Üí fine-tuned to 10)  \n",
    "**Hardware:** Kaggle H100 GPU (80GB VRAM)  \n",
    "\n",
    "### Why SegFormer?\n",
    "- **Hierarchical Transformer Encoder (MiT-B5):** Generates multi-scale features without positional encoding ‚Üí resolution-agnostic\n",
    "- **Lightweight All-MLP Decoder:** Drastically fewer params than ASPP/atrous convolutions (DeepLabv3+), yet higher mIoU\n",
    "- **State-of-the-art:** 84.0 mIoU on Cityscapes, outperforms DeepLabv3+, Swin-based methods & DINOv2 linear probes\n",
    "- **Transfer learning friendly:** ADE20K pretrained weights transfer beautifully to off-road terrain segmentation\n",
    "\n",
    "### References\n",
    "- [1] Xie et al., \"SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers,\" NeurIPS 2021\n",
    "- [2] Chen et al., \"Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,\" ECCV 2018\n",
    "- [3] Maturana et al., \"Real-time Semantic Mapping for Autonomous Off-Road Navigation,\" 2017\n",
    "- [4-6] Bozinovski (1976), Bozinovski (2020), Pan & Yang (2010) ‚Äî Transfer Learning foundations\n",
    "- [7] Minhas, \"Transfer Learning for Semantic Segmentation using PyTorch DeepLabv3,\" 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9543e361",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup & Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3323ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q transformers accelerate datasets evaluate segmentation-models-pytorch albumentations timm\n",
    "!pip install -q matplotlib seaborn tqdm pillow opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574da2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "import shutil\n",
    "import zipfile\n",
    "import random\n",
    "import warnings\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from transformers import (\n",
    "    SegformerForSemanticSegmentation,\n",
    "    SegformerConfig,\n",
    "    SegformerImageProcessor,\n",
    ")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Reproducibility ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = False  # H100: allow TF32\n",
    "torch.backends.cudnn.benchmark = True       # H100: auto-tune kernels\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Device ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è  Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
    "    # Enable TF32 for H100 (massive speedup)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    print(f\"   TF32: Enabled ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3885d2a",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b2df79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "#  MASTER CONFIGURATION ‚Äî Paths matched to Kaggle dataset structure\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "class CFG:\n",
    "    # ‚îÄ‚îÄ Paths (Kaggle ‚Äî already extracted, double-nested) ‚îÄ‚îÄ\n",
    "    # Dataset name on Kaggle: \"hack_for_bharat\"\n",
    "    DATA_ROOT        = '/kaggle/input/hack-for-bharat'\n",
    "    \n",
    "    # Training data (double-nested folder structure)\n",
    "    TRAIN_DIR        = os.path.join(DATA_ROOT, 'Offroad_Segmentation_Training_Dataset',\n",
    "                                    'Offroad_Segmentation_Training_Dataset', 'train')\n",
    "    VAL_DIR          = os.path.join(DATA_ROOT, 'Offroad_Segmentation_Training_Dataset',\n",
    "                                    'Offroad_Segmentation_Training_Dataset', 'val')\n",
    "    TEST_DIR         = os.path.join(DATA_ROOT, 'Offroad_Segmentation_testImages',\n",
    "                                    'Offroad_Segmentation_testImages')\n",
    "    \n",
    "    OUTPUT_DIR       = '/kaggle/working/outputs'\n",
    "    CHECKPOINT_DIR   = '/kaggle/working/checkpoints'\n",
    "    RESULTS_DIR      = '/kaggle/working/results'\n",
    "    \n",
    "    # ‚îÄ‚îÄ Model ‚îÄ‚îÄ\n",
    "    MODEL_NAME       = 'nvidia/segformer-b5-finetuned-ade-640-640'\n",
    "    NUM_CLASSES      = 10\n",
    "    \n",
    "    # ‚îÄ‚îÄ Training ‚îÄ‚îÄ\n",
    "    IMG_SIZE         = 640          # SegFormer works best at 640\n",
    "    BATCH_SIZE       = 8            # H100 80GB can handle this at 640x640\n",
    "    ACCUMULATION     = 2            # Effective batch = 16\n",
    "    NUM_EPOCHS       = 100          # Will early-stop much sooner\n",
    "    LR               = 2e-4         # AdamW peak LR\n",
    "    MIN_LR           = 1e-7         # Cosine annealing floor\n",
    "    WEIGHT_DECAY     = 0.01\n",
    "    WARMUP_EPOCHS    = 3\n",
    "    \n",
    "    # ‚îÄ‚îÄ Early Stopping ‚îÄ‚îÄ\n",
    "    PATIENCE_MIOU    = 15           # Stop if val mIoU doesn't improve for N epochs\n",
    "    PATIENCE_LOSS    = 20           # Stop if val loss doesn't improve for N epochs\n",
    "    MIN_DELTA        = 1e-4         # Minimum improvement threshold\n",
    "    \n",
    "    # ‚îÄ‚îÄ Checkpointing ‚îÄ‚îÄ\n",
    "    SAVE_EVERY       = 5            # Save checkpoint every N epochs\n",
    "    KEEP_TOP_K       = 3            # Keep top K best models\n",
    "    \n",
    "    # ‚îÄ‚îÄ Augmentation ‚îÄ‚îÄ\n",
    "    USE_MIXUP        = False        # Mixup augmentation\n",
    "    AUG_PROB         = 0.5          # Probability of each augmentation\n",
    "    \n",
    "    # ‚îÄ‚îÄ Mixed Precision ‚îÄ‚îÄ\n",
    "    USE_AMP          = True         # FP16/BF16 on H100\n",
    "    \n",
    "    # ‚îÄ‚îÄ Workers ‚îÄ‚îÄ\n",
    "    NUM_WORKERS      = 4\n",
    "    PIN_MEMORY       = True\n",
    "\n",
    "# Create directories\n",
    "for d in [CFG.OUTPUT_DIR, CFG.CHECKPOINT_DIR, CFG.RESULTS_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in vars(CFG).items():\n",
    "    if not k.startswith('_'):\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11469b7b",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Verify Dataset Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ccb783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "#  VERIFY DATASET PATHS (already extracted on Kaggle)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# Kaggle dataset structure (double-nested):\n",
    "# /kaggle/input/hack-for-bharat/\n",
    "#   ‚îú‚îÄ‚îÄ Offroad_Segmentation_Scripts/\n",
    "#   ‚îú‚îÄ‚îÄ Offroad_Segmentation_Training_Dataset/\n",
    "#   ‚îÇ   ‚îî‚îÄ‚îÄ Offroad_Segmentation_Training_Dataset/\n",
    "#   ‚îÇ       ‚îú‚îÄ‚îÄ train/\n",
    "#   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ Color_Images/\n",
    "#   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ Segmentation/\n",
    "#   ‚îÇ       ‚îî‚îÄ‚îÄ val/\n",
    "#   ‚îÇ           ‚îú‚îÄ‚îÄ Color_Images/\n",
    "#   ‚îÇ           ‚îî‚îÄ‚îÄ Segmentation/\n",
    "#   ‚îî‚îÄ‚îÄ Offroad_Segmentation_testImages/\n",
    "#       ‚îî‚îÄ‚îÄ Offroad_Segmentation_testImages/\n",
    "#           ‚îú‚îÄ‚îÄ Color_Images/\n",
    "#           ‚îî‚îÄ‚îÄ Segmentation/\n",
    "\n",
    "TRAIN_DIR = CFG.TRAIN_DIR\n",
    "VAL_DIR   = CFG.VAL_DIR\n",
    "TEST_DIR  = CFG.TEST_DIR\n",
    "\n",
    "# Auto-detect: try common Kaggle dataset name variants\n",
    "if not os.path.exists(TRAIN_DIR):\n",
    "    print(\"‚ö†Ô∏è  Default paths not found, auto-detecting...\")\n",
    "    # Search for the train/Color_Images folder anywhere under /kaggle/input\n",
    "    import subprocess\n",
    "    candidates = glob.glob('/kaggle/input/**/train/Color_Images', recursive=True)\n",
    "    if candidates:\n",
    "        TRAIN_DIR = os.path.dirname(candidates[0])               # .../train\n",
    "        VAL_DIR   = os.path.join(os.path.dirname(TRAIN_DIR), 'val')\n",
    "        print(f\"   Found TRAIN_DIR: {TRAIN_DIR}\")\n",
    "        print(f\"   Found VAL_DIR:   {VAL_DIR}\")\n",
    "    \n",
    "    test_candidates = glob.glob('/kaggle/input/**/Offroad_Segmentation_testImages/**/Color_Images', recursive=True)\n",
    "    if test_candidates:\n",
    "        TEST_DIR = os.path.dirname(test_candidates[0])\n",
    "        print(f\"   Found TEST_DIR:  {TEST_DIR}\")\n",
    "\n",
    "# Verify\n",
    "print(\"\\nüìÇ Dataset Paths:\")\n",
    "for name, path in [('Train', TRAIN_DIR), ('Val', VAL_DIR), ('Test', TEST_DIR)]:\n",
    "    img_dir = os.path.join(path, 'Color_Images')\n",
    "    seg_dir = os.path.join(path, 'Segmentation')\n",
    "    n_img = len(os.listdir(img_dir)) if os.path.exists(img_dir) else 0\n",
    "    n_seg = len(os.listdir(seg_dir)) if os.path.exists(seg_dir) else 0\n",
    "    status = \"‚úÖ\" if n_img > 0 else \"‚ùå\"\n",
    "    print(f\"  {status} {name:5s}: {n_img} images, {n_seg} masks  ‚Üí  {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9985d2d8",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Class Definitions & Color Palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c228faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "#  CLASS MAPPING ‚Äî From raw mask pixel values to class IDs\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "VALUE_MAP = {\n",
    "    0    : 0,   # Background\n",
    "    100  : 1,   # Trees\n",
    "    200  : 2,   # Lush Bushes\n",
    "    300  : 3,   # Dry Grass\n",
    "    500  : 4,   # Dry Bushes\n",
    "    550  : 5,   # Ground Clutter\n",
    "    700  : 6,   # Logs\n",
    "    800  : 7,   # Rocks\n",
    "    7100 : 8,   # Landscape\n",
    "    10000: 9,   # Sky\n",
    "}\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    'Background', 'Trees', 'Lush Bushes', 'Dry Grass', 'Dry Bushes',\n",
    "    'Ground Clutter', 'Logs', 'Rocks', 'Landscape', 'Sky'\n",
    "]\n",
    "\n",
    "# Beautiful color palette for visualization\n",
    "COLOR_PALETTE = np.array([\n",
    "    [  0,   0,   0],   # Background  ‚Äî black\n",
    "    [ 34, 139,  34],   # Trees       ‚Äî forest green\n",
    "    [  0, 255,   0],   # Lush Bushes ‚Äî lime\n",
    "    [210, 180, 140],   # Dry Grass   ‚Äî tan\n",
    "    [139,  90,  43],   # Dry Bushes  ‚Äî brown\n",
    "    [128, 128,   0],   # Ground Clutter ‚Äî olive\n",
    "    [139,  69,  19],   # Logs        ‚Äî saddle brown\n",
    "    [128, 128, 128],   # Rocks       ‚Äî gray\n",
    "    [160,  82,  45],   # Landscape   ‚Äî sienna\n",
    "    [135, 206, 235],   # Sky         ‚Äî sky blue\n",
    "], dtype=np.uint8)\n",
    "\n",
    "ID2LABEL = {i: name for i, name in enumerate(CLASS_NAMES)}\n",
    "LABEL2ID = {name: i for i, name in enumerate(CLASS_NAMES)}\n",
    "\n",
    "print(f\"Number of classes: {CFG.NUM_CLASSES}\")\n",
    "for i, name in enumerate(CLASS_NAMES):\n",
    "    print(f\"  {i}: {name} (color: {COLOR_PALETTE[i].tolist()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb99210",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Dataset & Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fd5c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mask(mask_np):\n",
    "    \"\"\"Convert raw mask pixel values (0, 100, 200, ..., 10000) ‚Üí class IDs (0-9).\"\"\"\n",
    "    out = np.zeros_like(mask_np, dtype=np.uint8)\n",
    "    for raw_val, class_id in VALUE_MAP.items():\n",
    "        out[mask_np == raw_val] = class_id\n",
    "    return out\n",
    "\n",
    "\n",
    "def mask_to_color(mask_np):\n",
    "    \"\"\"Convert class ID mask to RGB visualization.\"\"\"\n",
    "    h, w = mask_np.shape\n",
    "    color = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    for cid in range(CFG.NUM_CLASSES):\n",
    "        color[mask_np == cid] = COLOR_PALETTE[cid]\n",
    "    return color\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ Albumentations pipelines (compatible with albumentations >= 2.0) ‚îÄ‚îÄ\n",
    "def get_train_transforms(img_size=CFG.IMG_SIZE):\n",
    "    return A.Compose([\n",
    "        # Resize first, then random scale + crop for multi-scale training\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.RandomScale(scale_limit=(-0.5, 0.5), p=0.5),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size,\n",
    "                      border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=0),\n",
    "        A.RandomCrop(height=img_size, width=img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.1),\n",
    "        A.RandomRotate90(p=0.25),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15,\n",
    "                           p=CFG.AUG_PROB, border_mode=cv2.BORDER_CONSTANT),\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(var_limit=(10, 50), p=1),\n",
    "            A.GaussianBlur(blur_limit=(3, 7), p=1),\n",
    "            A.MotionBlur(blur_limit=7, p=1),\n",
    "        ], p=0.3),\n",
    "        A.OneOf([\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1),\n",
    "            A.HueSaturationValue(hue_shift_limit=15, sat_shift_limit=25, val_shift_limit=15, p=1),\n",
    "            A.CLAHE(clip_limit=4.0, p=1),\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=1),\n",
    "        ], p=CFG.AUG_PROB),\n",
    "        A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.2),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_val_transforms(img_size=CFG.IMG_SIZE):\n",
    "    return A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "\n",
    "class OffroadSegmentationDataset(Dataset):\n",
    "    \"\"\"Off-road terrain segmentation dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, transforms=None, is_test=False):\n",
    "        self.image_dir = os.path.join(data_dir, 'Color_Images')\n",
    "        self.mask_dir  = os.path.join(data_dir, 'Segmentation')\n",
    "        self.transforms = transforms\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        self.image_files = sorted(os.listdir(self.image_dir))\n",
    "        print(f\"  Loaded {len(self.image_files)} samples from {data_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.image_files[idx]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = os.path.join(self.image_dir, fname)\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Load mask\n",
    "        mask_path = os.path.join(self.mask_dir, fname)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        # Handle multi-channel masks\n",
    "        if mask is not None and len(mask.shape) == 3:\n",
    "            mask = mask[:, :, 0]\n",
    "        \n",
    "        # Convert raw values ‚Üí class IDs\n",
    "        mask = convert_mask(mask)\n",
    "        \n",
    "        # Apply augmentations\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=mask)\n",
    "            image = augmented['image']   # (C, H, W) float tensor\n",
    "            mask  = augmented['mask']    # (H, W) uint8\n",
    "        \n",
    "        mask = mask.long()\n",
    "        return image, mask, fname\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "print(\"Creating datasets...\")\n",
    "train_dataset = OffroadSegmentationDataset(TRAIN_DIR, transforms=get_train_transforms())\n",
    "val_dataset   = OffroadSegmentationDataset(VAL_DIR, transforms=get_val_transforms())\n",
    "test_dataset  = OffroadSegmentationDataset(TEST_DIR, transforms=get_val_transforms(), is_test=True)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=CFG.BATCH_SIZE, shuffle=True,\n",
    "    num_workers=CFG.NUM_WORKERS, pin_memory=CFG.PIN_MEMORY, drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=CFG.BATCH_SIZE, shuffle=False,\n",
    "    num_workers=CFG.NUM_WORKERS, pin_memory=CFG.PIN_MEMORY\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=CFG.BATCH_SIZE, shuffle=False,\n",
    "    num_workers=CFG.NUM_WORKERS, pin_memory=CFG.PIN_MEMORY\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Dataloaders created:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples ‚Üí {len(train_loader)} batches\")\n",
    "print(f\"  Val:   {len(val_dataset)} samples ‚Üí {len(val_loader)} batches\")\n",
    "print(f\"  Test:  {len(test_dataset)} samples ‚Üí {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88733367",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Visualize Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f9ecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(dataset, n=4, title='Samples'):\n",
    "    \"\"\"Visualize random image-mask pairs.\"\"\"\n",
    "    fig, axes = plt.subplots(n, 3, figsize=(15, 5*n))\n",
    "    if n == 1:\n",
    "        axes = axes[np.newaxis, :]\n",
    "    \n",
    "    indices = random.sample(range(len(dataset)), n)\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std  = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    for row, idx in enumerate(indices):\n",
    "        img, mask, fname = dataset[idx]\n",
    "        \n",
    "        # Denormalize image\n",
    "        img_np = img.numpy().transpose(1, 2, 0)\n",
    "        img_np = (img_np * std + mean) * 255\n",
    "        img_np = np.clip(img_np, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        # Mask\n",
    "        mask_np = mask.numpy().astype(np.uint8)\n",
    "        mask_color = mask_to_color(mask_np)\n",
    "        \n",
    "        # Overlay\n",
    "        overlay = cv2.addWeighted(img_np, 0.6, mask_color, 0.4, 0)\n",
    "        \n",
    "        axes[row, 0].imshow(img_np)\n",
    "        axes[row, 0].set_title(f'Image: {fname}')\n",
    "        axes[row, 0].axis('off')\n",
    "        \n",
    "        axes[row, 1].imshow(mask_color)\n",
    "        axes[row, 1].set_title('Ground Truth Mask')\n",
    "        axes[row, 1].axis('off')\n",
    "        \n",
    "        axes[row, 2].imshow(overlay)\n",
    "        axes[row, 2].set_title('Overlay')\n",
    "        axes[row, 2].axis('off')\n",
    "    \n",
    "    # Legend\n",
    "    patches = [mpatches.Patch(color=COLOR_PALETTE[i]/255., label=CLASS_NAMES[i]) for i in range(CFG.NUM_CLASSES)]\n",
    "    fig.legend(handles=patches, loc='lower center', ncol=5, fontsize=10, bbox_to_anchor=(0.5, -0.02))\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CFG.OUTPUT_DIR, 'sample_visualization.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "visualize_samples(train_dataset, n=4, title='Training Samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99f430f",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Class Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242ed5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(dataset, num_samples=200):\n",
    "    \"\"\"Compute class weights based on pixel frequency.\"\"\"\n",
    "    print(\"Computing class pixel frequencies...\")\n",
    "    pixel_counts = np.zeros(CFG.NUM_CLASSES, dtype=np.float64)\n",
    "    \n",
    "    indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n",
    "    for idx in tqdm(indices, desc='Scanning'):\n",
    "        _, mask, _ = dataset[idx]\n",
    "        mask_np = mask.numpy()\n",
    "        for c in range(CFG.NUM_CLASSES):\n",
    "            pixel_counts[c] += (mask_np == c).sum()\n",
    "    \n",
    "    total_pixels = pixel_counts.sum()\n",
    "    freq = pixel_counts / total_pixels\n",
    "    \n",
    "    # Inverse frequency weights (capped)\n",
    "    weights = 1.0 / (freq + 1e-6)\n",
    "    weights = weights / weights.sum() * CFG.NUM_CLASSES  # normalize so mean=1\n",
    "    weights = np.clip(weights, 0.5, 10.0)  # cap extreme weights\n",
    "    \n",
    "    print(\"\\nClass Distribution:\")\n",
    "    for i in range(CFG.NUM_CLASSES):\n",
    "        bar = '‚ñà' * int(freq[i] * 100)\n",
    "        print(f\"  {CLASS_NAMES[i]:<16}: {freq[i]*100:6.2f}%  {bar}  (weight: {weights[i]:.3f})\")\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    colors = [COLOR_PALETTE[i]/255. for i in range(CFG.NUM_CLASSES)]\n",
    "    \n",
    "    ax1.barh(CLASS_NAMES, freq * 100, color=colors, edgecolor='black')\n",
    "    ax1.set_xlabel('Pixel Percentage (%)')\n",
    "    ax1.set_title('Class Distribution')\n",
    "    \n",
    "    ax2.barh(CLASS_NAMES, weights, color=colors, edgecolor='black')\n",
    "    ax2.set_xlabel('Weight')\n",
    "    ax2.set_title('Class Weights (Inverse Frequency)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CFG.OUTPUT_DIR, 'class_distribution.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "class_weights = compute_class_weights(train_dataset, num_samples=300)\n",
    "print(f\"\\nClass weights tensor: {class_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dddbaff",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. SegFormer Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f7344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_segformer_model():\n",
    "    \"\"\"Build SegFormer-B5 with custom number of classes.\"\"\"\n",
    "    print(f\"üîß Loading SegFormer: {CFG.MODEL_NAME}\")\n",
    "    print(f\"   Fine-tuning for {CFG.NUM_CLASSES} classes\")\n",
    "    \n",
    "    model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "        CFG.MODEL_NAME,\n",
    "        num_labels=CFG.NUM_CLASSES,\n",
    "        id2label=ID2LABEL,\n",
    "        label2id=LABEL2ID,\n",
    "        ignore_mismatched_sizes=True,  # decoder head size changes\n",
    "    )\n",
    "    \n",
    "    # Count parameters\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"   Total params:     {total/1e6:.1f}M\")\n",
    "    print(f\"   Trainable params: {trainable/1e6:.1f}M\")\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "model = build_segformer_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f673a534",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Loss Functions, Metrics & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcf79f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "#  COMBINED LOSS: CrossEntropy + Dice + Focal\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred_soft = F.softmax(pred, dim=1)\n",
    "        target_oh = F.one_hot(target, self.num_classes).permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        intersection = (pred_soft * target_oh).sum(dim=(2, 3))\n",
    "        union = pred_soft.sum(dim=(2, 3)) + target_oh.sum(dim=(2, 3))\n",
    "        \n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1. - dice.mean()\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        ce_loss = F.cross_entropy(pred, target, weight=self.alpha, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        return focal_loss\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"CE + Dice + Focal combined loss for robust training.\"\"\"\n",
    "    def __init__(self, class_weights=None, ce_weight=1.0, dice_weight=1.0, focal_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.ce_weight = ce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        self.focal_weight = focal_weight\n",
    "        \n",
    "        w = class_weights.to(device) if class_weights is not None else None\n",
    "        self.ce_loss = nn.CrossEntropyLoss(weight=w)\n",
    "        self.dice_loss = DiceLoss(num_classes=CFG.NUM_CLASSES)\n",
    "        self.focal_loss = FocalLoss(alpha=w, gamma=2.0)\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        loss_ce    = self.ce_loss(pred, target)\n",
    "        loss_dice  = self.dice_loss(pred, target)\n",
    "        loss_focal = self.focal_loss(pred, target)\n",
    "        \n",
    "        total = (self.ce_weight * loss_ce +\n",
    "                 self.dice_weight * loss_dice +\n",
    "                 self.focal_weight * loss_focal)\n",
    "        return total, {\n",
    "            'ce': loss_ce.item(),\n",
    "            'dice': loss_dice.item(),\n",
    "            'focal': loss_focal.item(),\n",
    "        }\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ Metrics ‚îÄ‚îÄ\n",
    "def compute_iou(pred, target, num_classes=CFG.NUM_CLASSES, smooth=1e-6):\n",
    "    \"\"\"Compute per-class IoU and mean IoU.\"\"\"\n",
    "    pred_cls = pred.argmax(dim=1).view(-1)\n",
    "    target_flat = target.view(-1)\n",
    "    \n",
    "    iou_per_class = []\n",
    "    for c in range(num_classes):\n",
    "        pred_c = (pred_cls == c)\n",
    "        tgt_c  = (target_flat == c)\n",
    "        inter  = (pred_c & tgt_c).sum().float()\n",
    "        union  = (pred_c | tgt_c).sum().float()\n",
    "        if union == 0:\n",
    "            iou_per_class.append(float('nan'))\n",
    "        else:\n",
    "            iou_per_class.append((inter / (union + smooth)).item())\n",
    "    \n",
    "    return np.nanmean(iou_per_class), iou_per_class\n",
    "\n",
    "\n",
    "def compute_dice(pred, target, num_classes=CFG.NUM_CLASSES, smooth=1e-6):\n",
    "    \"\"\"Compute per-class Dice and mean Dice.\"\"\"\n",
    "    pred_cls = pred.argmax(dim=1).view(-1)\n",
    "    target_flat = target.view(-1)\n",
    "    \n",
    "    dice_per_class = []\n",
    "    for c in range(num_classes):\n",
    "        pred_c = (pred_cls == c)\n",
    "        tgt_c  = (target_flat == c)\n",
    "        inter  = (pred_c & tgt_c).sum().float()\n",
    "        dice = (2. * inter + smooth) / (pred_c.sum().float() + tgt_c.sum().float() + smooth)\n",
    "        dice_per_class.append(dice.item())\n",
    "    \n",
    "    return np.mean(dice_per_class), dice_per_class\n",
    "\n",
    "\n",
    "def compute_pixel_accuracy(pred, target):\n",
    "    \"\"\"Compute overall pixel accuracy.\"\"\"\n",
    "    pred_cls = pred.argmax(dim=1)\n",
    "    return (pred_cls == target).float().mean().item()\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ Instantiate ‚îÄ‚îÄ\n",
    "criterion = CombinedLoss(class_weights=class_weights, ce_weight=1.0, dice_weight=1.0, focal_weight=0.5)\n",
    "print(\"‚úÖ Combined Loss: CrossEntropy + Dice + Focal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1467d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "#  OPTIMIZER & SCHEDULER\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# Layer-wise learning rate decay for transformer\n",
    "def get_parameter_groups(model, lr=CFG.LR, wd=CFG.WEIGHT_DECAY, lr_decay=0.9):\n",
    "    \"\"\"Apply layer-wise LR decay: earlier layers get lower LR.\"\"\"\n",
    "    no_decay = ['bias', 'LayerNorm.weight', 'layer_norm.weight']\n",
    "    \n",
    "    # Encoder layers ‚Üí decayed LR\n",
    "    encoder_params_decay = []\n",
    "    encoder_params_no_decay = []\n",
    "    decoder_params_decay = []\n",
    "    decoder_params_no_decay = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        is_no_decay = any(nd in name for nd in no_decay)\n",
    "        \n",
    "        if 'encoder' in name or 'segformer.encoder' in name:\n",
    "            if is_no_decay:\n",
    "                encoder_params_no_decay.append(param)\n",
    "            else:\n",
    "                encoder_params_decay.append(param)\n",
    "        else:\n",
    "            if is_no_decay:\n",
    "                decoder_params_no_decay.append(param)\n",
    "            else:\n",
    "                decoder_params_decay.append(param)\n",
    "    \n",
    "    param_groups = [\n",
    "        {'params': encoder_params_decay,     'lr': lr * 0.1, 'weight_decay': wd},\n",
    "        {'params': encoder_params_no_decay,  'lr': lr * 0.1, 'weight_decay': 0.0},\n",
    "        {'params': decoder_params_decay,     'lr': lr,       'weight_decay': wd},\n",
    "        {'params': decoder_params_no_decay,  'lr': lr,       'weight_decay': 0.0},\n",
    "    ]\n",
    "    \n",
    "    print(f\"  Encoder params (with decay):    {len(encoder_params_decay)} tensors, LR={lr*0.1:.1e}\")\n",
    "    print(f\"  Encoder params (no decay):      {len(encoder_params_no_decay)} tensors, LR={lr*0.1:.1e}\")\n",
    "    print(f\"  Decoder params (with decay):    {len(decoder_params_decay)} tensors, LR={lr:.1e}\")\n",
    "    print(f\"  Decoder params (no decay):      {len(decoder_params_no_decay)} tensors, LR={lr:.1e}\")\n",
    "    \n",
    "    return param_groups\n",
    "\n",
    "\n",
    "param_groups = get_parameter_groups(model)\n",
    "optimizer = optim.AdamW(param_groups, lr=CFG.LR, weight_decay=CFG.WEIGHT_DECAY)\n",
    "\n",
    "# Cosine Annealing with Warm Restarts\n",
    "total_steps = len(train_loader) * CFG.NUM_EPOCHS // CFG.ACCUMULATION\n",
    "warmup_steps = len(train_loader) * CFG.WARMUP_EPOCHS // CFG.ACCUMULATION\n",
    "\n",
    "def cosine_warmup_scheduler(optimizer, warmup_steps, total_steps, min_lr=CFG.MIN_LR):\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return float(step) / float(max(1, warmup_steps))\n",
    "        progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        return max(min_lr / CFG.LR, 0.5 * (1.0 + np.cos(np.pi * progress)))\n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "scheduler = cosine_warmup_scheduler(optimizer, warmup_steps, total_steps)\n",
    "scaler = GradScaler(enabled=CFG.USE_AMP)\n",
    "\n",
    "print(f\"\\n‚úÖ Optimizer: AdamW (layerwise LR decay)\")\n",
    "print(f\"‚úÖ Scheduler: Cosine with {CFG.WARMUP_EPOCHS} warmup epochs\")\n",
    "print(f\"‚úÖ AMP: {'Enabled' if CFG.USE_AMP else 'Disabled'}\")\n",
    "print(f\"   Total steps: {total_steps}, Warmup steps: {warmup_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a5d7e4",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Early Stopping & Checkpoint Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a86dcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Multi-metric early stopping with patience.\"\"\"\n",
    "    \n",
    "    def __init__(self, patience_miou=CFG.PATIENCE_MIOU, patience_loss=CFG.PATIENCE_LOSS,\n",
    "                 min_delta=CFG.MIN_DELTA):\n",
    "        self.patience_miou = patience_miou\n",
    "        self.patience_loss = patience_loss\n",
    "        self.min_delta = min_delta\n",
    "        \n",
    "        self.best_miou = -np.inf\n",
    "        self.best_loss = np.inf\n",
    "        self.miou_counter = 0\n",
    "        self.loss_counter = 0\n",
    "        self.should_stop = False\n",
    "        self.best_epoch_miou = 0\n",
    "        self.best_epoch_loss = 0\n",
    "    \n",
    "    def __call__(self, epoch, val_miou, val_loss):\n",
    "        improved = False\n",
    "        \n",
    "        # Check mIoU improvement\n",
    "        if val_miou > self.best_miou + self.min_delta:\n",
    "            self.best_miou = val_miou\n",
    "            self.miou_counter = 0\n",
    "            self.best_epoch_miou = epoch\n",
    "            improved = True\n",
    "        else:\n",
    "            self.miou_counter += 1\n",
    "        \n",
    "        # Check loss improvement\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.loss_counter = 0\n",
    "            self.best_epoch_loss = epoch\n",
    "        else:\n",
    "            self.loss_counter += 1\n",
    "        \n",
    "        # Stop if BOTH metrics stagnated\n",
    "        if self.miou_counter >= self.patience_miou and self.loss_counter >= self.patience_loss:\n",
    "            self.should_stop = True\n",
    "            print(f\"\\nüõë EARLY STOPPING at epoch {epoch+1}\")\n",
    "            print(f\"   mIoU: no improvement for {self.miou_counter} epochs (best: {self.best_miou:.4f} at epoch {self.best_epoch_miou+1})\")\n",
    "            print(f\"   Loss: no improvement for {self.loss_counter} epochs (best: {self.best_loss:.4f} at epoch {self.best_epoch_loss+1})\")\n",
    "        \n",
    "        return improved\n",
    "    \n",
    "    def status(self):\n",
    "        return (f\"mIoU patience: {self.miou_counter}/{self.patience_miou} | \"\n",
    "                f\"Loss patience: {self.loss_counter}/{self.patience_loss}\")\n",
    "\n",
    "\n",
    "class CheckpointManager:\n",
    "    \"\"\"Manages model checkpoints: best, periodic & top-K.\"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_dir, keep_top_k=CFG.KEEP_TOP_K):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.keep_top_k = keep_top_k\n",
    "        self.best_models = []  # (miou, path) sorted ascending\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    def save_checkpoint(self, model, optimizer, scheduler, scaler, epoch, metrics, is_best=False):\n",
    "        \"\"\"Save a full training checkpoint (resumable).\"\"\"\n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict() if scaler else None,\n",
    "            'metrics': metrics,\n",
    "            'config': {k: v for k, v in vars(CFG).items() if not k.startswith('_')},\n",
    "        }\n",
    "        \n",
    "        # Save periodic checkpoint\n",
    "        path = os.path.join(self.checkpoint_dir, f'checkpoint_epoch_{epoch+1:03d}.pt')\n",
    "        torch.save(state, path)\n",
    "        \n",
    "        # Save best model\n",
    "        if is_best:\n",
    "            best_path = os.path.join(self.checkpoint_dir, 'best_model.pt')\n",
    "            torch.save(state, best_path)\n",
    "            print(f\"  üíæ Saved BEST model (mIoU: {metrics['val_miou']:.4f})\")\n",
    "            \n",
    "            # Also save HuggingFace-format for easy loading\n",
    "            hf_path = os.path.join(self.checkpoint_dir, 'best_model_hf')\n",
    "            model.save_pretrained(hf_path)\n",
    "        \n",
    "        # Top-K management\n",
    "        miou = metrics.get('val_miou', 0)\n",
    "        self.best_models.append((miou, path))\n",
    "        self.best_models.sort(key=lambda x: x[0])\n",
    "        \n",
    "        while len(self.best_models) > self.keep_top_k:\n",
    "            _, remove_path = self.best_models.pop(0)\n",
    "            if os.path.exists(remove_path) and 'best_model' not in remove_path:\n",
    "                os.remove(remove_path)\n",
    "        \n",
    "        return path\n",
    "    \n",
    "    def save_last(self, model, optimizer, scheduler, scaler, epoch, metrics):\n",
    "        \"\"\"Always save 'last' checkpoint for resuming.\"\"\"\n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict() if scaler else None,\n",
    "            'metrics': metrics,\n",
    "        }\n",
    "        path = os.path.join(self.checkpoint_dir, 'last_checkpoint.pt')\n",
    "        torch.save(state, path)\n",
    "        return path\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_checkpoint(path, model, optimizer=None, scheduler=None, scaler=None):\n",
    "        \"\"\"Load checkpoint and optionally restore optimizer/scheduler.\"\"\"\n",
    "        print(f\"üìÇ Loading checkpoint: {path}\")\n",
    "        ckpt = torch.load(path, map_location=device)\n",
    "        model.load_state_dict(ckpt['model_state_dict'])\n",
    "        \n",
    "        if optimizer and 'optimizer_state_dict' in ckpt:\n",
    "            optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "        if scheduler and 'scheduler_state_dict' in ckpt:\n",
    "            scheduler.load_state_dict(ckpt['scheduler_state_dict'])\n",
    "        if scaler and ckpt.get('scaler_state_dict'):\n",
    "            scaler.load_state_dict(ckpt['scaler_state_dict'])\n",
    "        \n",
    "        epoch = ckpt.get('epoch', 0)\n",
    "        metrics = ckpt.get('metrics', {})\n",
    "        print(f\"   Resumed from epoch {epoch+1}, val_miou={metrics.get('val_miou', 'N/A')}\")\n",
    "        return epoch, metrics\n",
    "\n",
    "\n",
    "# Instantiate\n",
    "early_stopping = EarlyStopping()\n",
    "ckpt_manager = CheckpointManager(CFG.CHECKPOINT_DIR)\n",
    "print(\"‚úÖ Early Stopping (dual mIoU+Loss) & Checkpoint Manager initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741e9bd2",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Training Loop (God-Level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda1d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, scheduler, scaler, epoch):\n",
    "    \"\"\"Train for one epoch with gradient accumulation & AMP.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_ce = 0.0\n",
    "    running_dice = 0.0\n",
    "    running_focal = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    # Metrics accumulators\n",
    "    all_iou = []\n",
    "    all_dice = []\n",
    "    all_acc = []\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    pbar = tqdm(loader, desc=f'Epoch {epoch+1} [Train]', leave=False)\n",
    "    \n",
    "    for step, (images, masks, _) in enumerate(pbar):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        masks  = masks.to(device, non_blocking=True)\n",
    "        \n",
    "        with autocast(enabled=CFG.USE_AMP):\n",
    "            outputs = model(pixel_values=images)\n",
    "            logits = outputs.logits  # (B, num_classes, H/4, W/4)\n",
    "            \n",
    "            # Upsample to original size\n",
    "            logits_up = F.interpolate(logits, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n",
    "            \n",
    "            loss, loss_dict = criterion(logits_up, masks)\n",
    "            loss = loss / CFG.ACCUMULATION\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (step + 1) % CFG.ACCUMULATION == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Track losses\n",
    "        running_loss += loss.item() * CFG.ACCUMULATION\n",
    "        running_ce += loss_dict['ce']\n",
    "        running_dice += loss_dict['dice']\n",
    "        running_focal += loss_dict['focal']\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Compute metrics on this batch\n",
    "        with torch.no_grad():\n",
    "            miou, _ = compute_iou(logits_up, masks)\n",
    "            mdice, _ = compute_dice(logits_up, masks)\n",
    "            acc = compute_pixel_accuracy(logits_up, masks)\n",
    "            all_iou.append(miou)\n",
    "            all_dice.append(mdice)\n",
    "            all_acc.append(acc)\n",
    "        \n",
    "        # Progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{running_loss/num_batches:.4f}',\n",
    "            'mIoU': f'{np.mean(all_iou):.3f}',\n",
    "            'lr': f'{scheduler.get_last_lr()[0]:.2e}',\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        'loss': running_loss / num_batches,\n",
    "        'ce_loss': running_ce / num_batches,\n",
    "        'dice_loss': running_dice / num_batches,\n",
    "        'focal_loss': running_focal / num_batches,\n",
    "        'miou': np.mean(all_iou),\n",
    "        'dice': np.mean(all_dice),\n",
    "        'pixel_acc': np.mean(all_acc),\n",
    "        'lr': scheduler.get_last_lr()[0],\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "    all_iou = []\n",
    "    all_dice = []\n",
    "    all_acc = []\n",
    "    all_class_iou = []\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Validating', leave=False)\n",
    "    for images, masks, _ in pbar:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        masks  = masks.to(device, non_blocking=True)\n",
    "        \n",
    "        with autocast(enabled=CFG.USE_AMP):\n",
    "            outputs = model(pixel_values=images)\n",
    "            logits = outputs.logits\n",
    "            logits_up = F.interpolate(logits, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n",
    "            loss, _ = criterion(logits_up, masks)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        miou, class_iou = compute_iou(logits_up, masks)\n",
    "        mdice, _ = compute_dice(logits_up, masks)\n",
    "        acc = compute_pixel_accuracy(logits_up, masks)\n",
    "        \n",
    "        all_iou.append(miou)\n",
    "        all_dice.append(mdice)\n",
    "        all_acc.append(acc)\n",
    "        all_class_iou.append(class_iou)\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{running_loss/num_batches:.4f}', 'mIoU': f'{miou:.3f}'})\n",
    "    \n",
    "    avg_class_iou = np.nanmean(all_class_iou, axis=0)\n",
    "    \n",
    "    return {\n",
    "        'loss': running_loss / num_batches,\n",
    "        'miou': np.mean(all_iou),\n",
    "        'dice': np.mean(all_dice),\n",
    "        'pixel_acc': np.mean(all_acc),\n",
    "        'class_iou': avg_class_iou,\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Training and validation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4018a3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "#  MAIN TRAINING LOOP\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "history = {\n",
    "    'train_loss': [], 'val_loss': [],\n",
    "    'train_miou': [], 'val_miou': [],\n",
    "    'train_dice': [], 'val_dice': [],\n",
    "    'train_pixel_acc': [], 'val_pixel_acc': [],\n",
    "    'train_ce': [], 'train_dice_loss': [], 'train_focal': [],\n",
    "    'lr': [],\n",
    "}\n",
    "\n",
    "best_miou = 0.0\n",
    "start_epoch = 0\n",
    "\n",
    "# ‚îÄ‚îÄ Resume from checkpoint if exists ‚îÄ‚îÄ\n",
    "resume_path = os.path.join(CFG.CHECKPOINT_DIR, 'last_checkpoint.pt')\n",
    "if os.path.exists(resume_path):\n",
    "    print(\"üîÑ Resuming from last checkpoint...\")\n",
    "    start_epoch, prev_metrics = CheckpointManager.load_checkpoint(\n",
    "        resume_path, model, optimizer, scheduler, scaler\n",
    "    )\n",
    "    start_epoch += 1\n",
    "    best_miou = prev_metrics.get('val_miou', 0)\n",
    "    early_stopping.best_miou = best_miou\n",
    "    # Load history if saved\n",
    "    hist_path = os.path.join(CFG.OUTPUT_DIR, 'training_history.json')\n",
    "    if os.path.exists(hist_path):\n",
    "        with open(hist_path, 'r') as f:\n",
    "            history = json.load(f)\n",
    "        print(f\"   Loaded training history ({len(history['train_loss'])} epochs)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"  TRAINING: SegFormer-B5 | {CFG.NUM_EPOCHS} epochs | BS={CFG.BATCH_SIZE}x{CFG.ACCUMULATION}={CFG.BATCH_SIZE*CFG.ACCUMULATION}\")\n",
    "print(f\"  Starting from epoch {start_epoch+1}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "training_start = time.time()\n",
    "\n",
    "for epoch in range(start_epoch, CFG.NUM_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # ‚îÄ‚îÄ Train ‚îÄ‚îÄ\n",
    "    train_metrics = train_one_epoch(model, train_loader, criterion, optimizer, scheduler, scaler, epoch)\n",
    "    \n",
    "    # ‚îÄ‚îÄ Validate ‚îÄ‚îÄ\n",
    "    val_metrics = validate(model, val_loader, criterion)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    # ‚îÄ‚îÄ Record history ‚îÄ‚îÄ\n",
    "    history['train_loss'].append(train_metrics['loss'])\n",
    "    history['val_loss'].append(val_metrics['loss'])\n",
    "    history['train_miou'].append(train_metrics['miou'])\n",
    "    history['val_miou'].append(val_metrics['miou'])\n",
    "    history['train_dice'].append(train_metrics['dice'])\n",
    "    history['val_dice'].append(val_metrics['dice'])\n",
    "    history['train_pixel_acc'].append(train_metrics['pixel_acc'])\n",
    "    history['val_pixel_acc'].append(val_metrics['pixel_acc'])\n",
    "    history['train_ce'].append(train_metrics['ce_loss'])\n",
    "    history['train_dice_loss'].append(train_metrics['dice_loss'])\n",
    "    history['train_focal'].append(train_metrics['focal_loss'])\n",
    "    history['lr'].append(train_metrics['lr'])\n",
    "    \n",
    "    # ‚îÄ‚îÄ Print epoch summary ‚îÄ‚îÄ\n",
    "    is_best = val_metrics['miou'] > best_miou\n",
    "    best_marker = ' ‚òÖ NEW BEST' if is_best else ''\n",
    "    if is_best:\n",
    "        best_miou = val_metrics['miou']\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1:3d}/{CFG.NUM_EPOCHS} ({epoch_time:.0f}s) | \"\n",
    "          f\"Train Loss: {train_metrics['loss']:.4f} | Val Loss: {val_metrics['loss']:.4f} | \"\n",
    "          f\"Train mIoU: {train_metrics['miou']:.4f} | Val mIoU: {val_metrics['miou']:.4f} | \"\n",
    "          f\"Val Dice: {val_metrics['dice']:.4f} | Val Acc: {val_metrics['pixel_acc']:.4f} | \"\n",
    "          f\"LR: {train_metrics['lr']:.2e}{best_marker}\")\n",
    "    \n",
    "    # Per-class IoU\n",
    "    if (epoch + 1) % 5 == 0 or is_best:\n",
    "        print(\"  Per-class IoU: \" + \" | \".join(\n",
    "            f\"{CLASS_NAMES[i]}: {val_metrics['class_iou'][i]:.3f}\" \n",
    "            for i in range(CFG.NUM_CLASSES) if not np.isnan(val_metrics['class_iou'][i])\n",
    "        ))\n",
    "    \n",
    "    # ‚îÄ‚îÄ Checkpointing ‚îÄ‚îÄ\n",
    "    epoch_metrics = {\n",
    "        'val_miou': val_metrics['miou'],\n",
    "        'val_loss': val_metrics['loss'],\n",
    "        'val_dice': val_metrics['dice'],\n",
    "        'val_pixel_acc': val_metrics['pixel_acc'],\n",
    "        'train_loss': train_metrics['loss'],\n",
    "    }\n",
    "    \n",
    "    # Always save last checkpoint (for resume)\n",
    "    ckpt_manager.save_last(model, optimizer, scheduler, scaler, epoch, epoch_metrics)\n",
    "    \n",
    "    # Save periodic / best checkpoints\n",
    "    if is_best or (epoch + 1) % CFG.SAVE_EVERY == 0:\n",
    "        ckpt_manager.save_checkpoint(\n",
    "            model, optimizer, scheduler, scaler, epoch, epoch_metrics, is_best=is_best\n",
    "        )\n",
    "    \n",
    "    # Save history to JSON (crash-safe)\n",
    "    with open(os.path.join(CFG.OUTPUT_DIR, 'training_history.json'), 'w') as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    \n",
    "    # ‚îÄ‚îÄ Early Stopping ‚îÄ‚îÄ\n",
    "    early_stopping(epoch, val_metrics['miou'], val_metrics['loss'])\n",
    "    print(f\"  Early stopping: {early_stopping.status()}\")\n",
    "    \n",
    "    if early_stopping.should_stop:\n",
    "        print(f\"\\nüõë Training stopped early at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "total_time = time.time() - training_start\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"  TRAINING COMPLETE in {total_time/3600:.2f} hours\")\n",
    "print(f\"  Best Val mIoU: {best_miou:.4f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf96b944",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Training Curves & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f69b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(history, output_dir):\n",
    "    \"\"\"Generate comprehensive training visualization plots.\"\"\"\n",
    "    n_epochs = len(history['train_loss'])\n",
    "    epochs = range(1, n_epochs + 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    \n",
    "    # 1. Loss\n",
    "    axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='Train', linewidth=2)\n",
    "    axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Val', linewidth=2)\n",
    "    axes[0, 0].set_title('Total Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].legend(fontsize=12)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. mIoU\n",
    "    axes[0, 1].plot(epochs, history['train_miou'], 'b-', label='Train', linewidth=2)\n",
    "    axes[0, 1].plot(epochs, history['val_miou'], 'r-', label='Val', linewidth=2)\n",
    "    best_idx = np.argmax(history['val_miou'])\n",
    "    axes[0, 1].axvline(x=best_idx+1, color='green', linestyle='--', alpha=0.7, label=f'Best: {history[\"val_miou\"][best_idx]:.4f}')\n",
    "    axes[0, 1].set_title('Mean IoU', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].legend(fontsize=12)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Dice\n",
    "    axes[0, 2].plot(epochs, history['train_dice'], 'b-', label='Train', linewidth=2)\n",
    "    axes[0, 2].plot(epochs, history['val_dice'], 'r-', label='Val', linewidth=2)\n",
    "    axes[0, 2].set_title('Dice Score', fontsize=14, fontweight='bold')\n",
    "    axes[0, 2].set_xlabel('Epoch')\n",
    "    axes[0, 2].legend(fontsize=12)\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Pixel Accuracy\n",
    "    axes[1, 0].plot(epochs, history['train_pixel_acc'], 'b-', label='Train', linewidth=2)\n",
    "    axes[1, 0].plot(epochs, history['val_pixel_acc'], 'r-', label='Val', linewidth=2)\n",
    "    axes[1, 0].set_title('Pixel Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].legend(fontsize=12)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Component Losses\n",
    "    axes[1, 1].plot(epochs, history['train_ce'], label='CE Loss', linewidth=2)\n",
    "    axes[1, 1].plot(epochs, history['train_dice_loss'], label='Dice Loss', linewidth=2)\n",
    "    axes[1, 1].plot(epochs, history['train_focal'], label='Focal Loss', linewidth=2)\n",
    "    axes[1, 1].set_title('Component Losses (Train)', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].legend(fontsize=12)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Learning Rate\n",
    "    axes[1, 2].plot(epochs, history['lr'], 'g-', linewidth=2)\n",
    "    axes[1, 2].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "    axes[1, 2].set_xlabel('Epoch')\n",
    "    axes[1, 2].set_yscale('log')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('SegFormer-B5 Training Curves', fontsize=18, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'training_curves_all.png'), dpi=200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved training curves to {output_dir}/training_curves_all.png\")\n",
    "\n",
    "plot_training_curves(history, CFG.OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a341ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed metrics report\n",
    "def save_metrics_report(history, output_dir):\n",
    "    \"\"\"Save comprehensive metrics to text file.\"\"\"\n",
    "    filepath = os.path.join(output_dir, 'evaluation_metrics.txt')\n",
    "    n_epochs = len(history['train_loss'])\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write(\"‚ïê\" * 100 + \"\\n\")\n",
    "        f.write(\"  SEGFORMER-B5 TRAINING REPORT\\n\")\n",
    "        f.write(f\"  Generated: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(\"‚ïê\" * 100 + \"\\n\\n\")\n",
    "        \n",
    "        # Best results\n",
    "        f.write(\"BEST RESULTS:\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "        f.write(f\"  Best Val mIoU:      {max(history['val_miou']):.4f} (Epoch {np.argmax(history['val_miou'])+1})\\n\")\n",
    "        f.write(f\"  Best Val Dice:      {max(history['val_dice']):.4f} (Epoch {np.argmax(history['val_dice'])+1})\\n\")\n",
    "        f.write(f\"  Best Val Accuracy:  {max(history['val_pixel_acc']):.4f} (Epoch {np.argmax(history['val_pixel_acc'])+1})\\n\")\n",
    "        f.write(f\"  Lowest Val Loss:    {min(history['val_loss']):.4f} (Epoch {np.argmin(history['val_loss'])+1})\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        # Final results\n",
    "        f.write(\"FINAL EPOCH RESULTS:\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "        f.write(f\"  Train Loss:     {history['train_loss'][-1]:.4f}\\n\")\n",
    "        f.write(f\"  Val Loss:       {history['val_loss'][-1]:.4f}\\n\")\n",
    "        f.write(f\"  Train mIoU:     {history['train_miou'][-1]:.4f}\\n\")\n",
    "        f.write(f\"  Val mIoU:       {history['val_miou'][-1]:.4f}\\n\")\n",
    "        f.write(f\"  Train Dice:     {history['train_dice'][-1]:.4f}\\n\")\n",
    "        f.write(f\"  Val Dice:       {history['val_dice'][-1]:.4f}\\n\")\n",
    "        f.write(f\"  Train Acc:      {history['train_pixel_acc'][-1]:.4f}\\n\")\n",
    "        f.write(f\"  Val Acc:        {history['val_pixel_acc'][-1]:.4f}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        # Per-epoch table\n",
    "        f.write(\"PER-EPOCH HISTORY:\\n\")\n",
    "        f.write(\"-\" * 120 + \"\\n\")\n",
    "        headers = ['Epoch', 'Train Loss', 'Val Loss', 'Train mIoU', 'Val mIoU', 'Train Dice', 'Val Dice', 'Train Acc', 'Val Acc', 'LR']\n",
    "        f.write(\"{:<8} {:<12} {:<12} {:<12} {:<12} {:<12} {:<12} {:<12} {:<12} {:<12}\\n\".format(*headers))\n",
    "        f.write(\"-\" * 120 + \"\\n\")\n",
    "        \n",
    "        for i in range(n_epochs):\n",
    "            f.write(\"{:<8} {:<12.4f} {:<12.4f} {:<12.4f} {:<12.4f} {:<12.4f} {:<12.4f} {:<12.4f} {:<12.4f} {:<12.2e}\\n\".format(\n",
    "                i+1,\n",
    "                history['train_loss'][i], history['val_loss'][i],\n",
    "                history['train_miou'][i], history['val_miou'][i],\n",
    "                history['train_dice'][i], history['val_dice'][i],\n",
    "                history['train_pixel_acc'][i], history['val_pixel_acc'][i],\n",
    "                history['lr'][i]\n",
    "            ))\n",
    "    \n",
    "    print(f\"üìù Saved metrics report to {filepath}\")\n",
    "\n",
    "save_metrics_report(history, CFG.OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b0f0c8",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Load Best Model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e22045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "#  LOAD BEST MODEL\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "best_ckpt_path = os.path.join(CFG.CHECKPOINT_DIR, 'best_model.pt')\n",
    "\n",
    "if os.path.exists(best_ckpt_path):\n",
    "    print(\"Loading best model for inference...\")\n",
    "    ckpt = torch.load(best_ckpt_path, map_location=device)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    print(f\"‚úÖ Best model loaded (Epoch {ckpt['epoch']+1}, Val mIoU: {ckpt['metrics']['val_miou']:.4f})\")\n",
    "else:\n",
    "    # Try loading last checkpoint\n",
    "    last_path = os.path.join(CFG.CHECKPOINT_DIR, 'last_checkpoint.pt')\n",
    "    if os.path.exists(last_path):\n",
    "        ckpt = torch.load(last_path, map_location=device)\n",
    "        model.load_state_dict(ckpt['model_state_dict'])\n",
    "        print(f\"‚úÖ Last checkpoint loaded (Epoch {ckpt['epoch']+1})\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No checkpoint found ‚Äî using current model state\")\n",
    "\n",
    "model.eval()\n",
    "print(\"Model set to eval mode ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ab6c43",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. Full Evaluation on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3260bd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def full_evaluation(model, loader, dataset_name='Validation'):\n",
    "    \"\"\"Run full evaluation with per-class metrics.\"\"\"\n",
    "    model.eval()\n",
    "    all_class_iou = []\n",
    "    all_class_dice = []\n",
    "    all_acc = []\n",
    "    \n",
    "    pbar = tqdm(loader, desc=f'Evaluating {dataset_name}')\n",
    "    for images, masks, _ in pbar:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        masks  = masks.to(device, non_blocking=True)\n",
    "        \n",
    "        with autocast(enabled=CFG.USE_AMP):\n",
    "            outputs = model(pixel_values=images)\n",
    "            logits_up = F.interpolate(outputs.logits, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        _, class_iou = compute_iou(logits_up, masks)\n",
    "        _, class_dice = compute_dice(logits_up, masks)\n",
    "        acc = compute_pixel_accuracy(logits_up, masks)\n",
    "        \n",
    "        all_class_iou.append(class_iou)\n",
    "        all_class_dice.append(class_dice)\n",
    "        all_acc.append(acc)\n",
    "    \n",
    "    avg_iou = np.nanmean(all_class_iou, axis=0)\n",
    "    avg_dice = np.nanmean(all_class_dice, axis=0)\n",
    "    mean_iou = np.nanmean(avg_iou)\n",
    "    mean_dice = np.mean(avg_dice)\n",
    "    mean_acc = np.mean(all_acc)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'‚ïê'*60}\")\n",
    "    print(f\"  {dataset_name.upper()} RESULTS\")\n",
    "    print(f\"{'‚ïê'*60}\")\n",
    "    print(f\"  Mean IoU:       {mean_iou:.4f}\")\n",
    "    print(f\"  Mean Dice:      {mean_dice:.4f}\")\n",
    "    print(f\"  Pixel Accuracy: {mean_acc:.4f}\")\n",
    "    print(f\"{'‚îÄ'*60}\")\n",
    "    print(f\"  {'Class':<20} {'IoU':>8} {'Dice':>8}\")\n",
    "    print(f\"  {'‚îÄ'*38}\")\n",
    "    for i in range(CFG.NUM_CLASSES):\n",
    "        iou_str = f\"{avg_iou[i]:.4f}\" if not np.isnan(avg_iou[i]) else 'N/A'\n",
    "        print(f\"  {CLASS_NAMES[i]:<20} {iou_str:>8} {avg_dice[i]:>8.4f}\")\n",
    "    print(f\"{'‚ïê'*60}\")\n",
    "    \n",
    "    # Bar chart\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    colors = [COLOR_PALETTE[i]/255. for i in range(CFG.NUM_CLASSES)]\n",
    "    \n",
    "    valid_iou = [v if not np.isnan(v) else 0 for v in avg_iou]\n",
    "    ax1.barh(CLASS_NAMES, valid_iou, color=colors, edgecolor='black')\n",
    "    ax1.axvline(x=mean_iou, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_iou:.4f}')\n",
    "    ax1.set_xlabel('IoU')\n",
    "    ax1.set_title(f'Per-Class IoU ({dataset_name})')\n",
    "    ax1.legend()\n",
    "    ax1.set_xlim(0, 1)\n",
    "    \n",
    "    ax2.barh(CLASS_NAMES, avg_dice, color=colors, edgecolor='black')\n",
    "    ax2.axvline(x=mean_dice, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_dice:.4f}')\n",
    "    ax2.set_xlabel('Dice Score')\n",
    "    ax2.set_title(f'Per-Class Dice ({dataset_name})')\n",
    "    ax2.legend()\n",
    "    ax2.set_xlim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CFG.OUTPUT_DIR, f'{dataset_name.lower()}_per_class_metrics.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return {'mean_iou': mean_iou, 'mean_dice': mean_dice, 'pixel_acc': mean_acc, 'class_iou': avg_iou, 'class_dice': avg_dice}\n",
    "\n",
    "val_results = full_evaluation(model, val_loader, 'Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da7d945",
   "metadata": {},
   "source": [
    "---\n",
    "## 15. Inference ‚Äî Validation Set Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0376c541",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_inference(model, loader, output_dir, save_comparisons=10, dataset_name='val'):\n",
    "    \"\"\"Run inference and save all prediction masks + visualizations.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    masks_dir = os.path.join(output_dir, f'{dataset_name}_masks')\n",
    "    color_dir = os.path.join(output_dir, f'{dataset_name}_masks_color')\n",
    "    comp_dir  = os.path.join(output_dir, f'{dataset_name}_comparisons')\n",
    "    os.makedirs(masks_dir, exist_ok=True)\n",
    "    os.makedirs(color_dir, exist_ok=True)\n",
    "    os.makedirs(comp_dir, exist_ok=True)\n",
    "    \n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std  = np.array([0.229, 0.224, 0.225])\n",
    "    sample_count = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=f'Inference ({dataset_name})')\n",
    "    for images, masks, fnames in pbar:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        masks  = masks.to(device, non_blocking=True)\n",
    "        \n",
    "        with autocast(enabled=CFG.USE_AMP):\n",
    "            outputs = model(pixel_values=images)\n",
    "            logits_up = F.interpolate(outputs.logits, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        preds = logits_up.argmax(dim=1).cpu().numpy().astype(np.uint8)\n",
    "        \n",
    "        for i in range(images.shape[0]):\n",
    "            fname = fnames[i]\n",
    "            base = os.path.splitext(fname)[0]\n",
    "            \n",
    "            # Save raw prediction mask\n",
    "            pred_mask = preds[i]\n",
    "            Image.fromarray(pred_mask).save(os.path.join(masks_dir, f'{base}_pred.png'))\n",
    "            \n",
    "            # Save colored prediction\n",
    "            pred_color = mask_to_color(pred_mask)\n",
    "            cv2.imwrite(os.path.join(color_dir, f'{base}_pred_color.png'),\n",
    "                       cv2.cvtColor(pred_color, cv2.COLOR_RGB2BGR))\n",
    "            \n",
    "            # Save comparison visualization\n",
    "            if sample_count < save_comparisons:\n",
    "                img_np = images[i].cpu().numpy().transpose(1, 2, 0)\n",
    "                img_np = ((img_np * std + mean) * 255).clip(0, 255).astype(np.uint8)\n",
    "                \n",
    "                gt_mask = masks[i].cpu().numpy().astype(np.uint8)\n",
    "                gt_color = mask_to_color(gt_mask)\n",
    "                \n",
    "                overlay = cv2.addWeighted(img_np, 0.5, pred_color, 0.5, 0)\n",
    "                \n",
    "                fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "                axes[0].imshow(img_np); axes[0].set_title('Input Image'); axes[0].axis('off')\n",
    "                axes[1].imshow(gt_color); axes[1].set_title('Ground Truth'); axes[1].axis('off')\n",
    "                axes[2].imshow(pred_color); axes[2].set_title('Prediction'); axes[2].axis('off')\n",
    "                axes[3].imshow(overlay); axes[3].set_title('Overlay'); axes[3].axis('off')\n",
    "                \n",
    "                patches = [mpatches.Patch(color=COLOR_PALETTE[c]/255., label=CLASS_NAMES[c]) for c in range(CFG.NUM_CLASSES)]\n",
    "                fig.legend(handles=patches, loc='lower center', ncol=5, fontsize=9, bbox_to_anchor=(0.5, -0.05))\n",
    "                \n",
    "                plt.suptitle(f'{fname}', fontsize=14)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(comp_dir, f'comparison_{sample_count:04d}.png'), dpi=150, bbox_inches='tight')\n",
    "                plt.close()\n",
    "            \n",
    "            sample_count += 1\n",
    "    \n",
    "    print(f\"\\n‚úÖ Inference complete: {sample_count} images processed\")\n",
    "    print(f\"   Masks:       {masks_dir}\")\n",
    "    print(f\"   Color masks: {color_dir}\")\n",
    "    print(f\"   Comparisons: {comp_dir}\")\n",
    "    return sample_count\n",
    "\n",
    "# Run on validation set\n",
    "run_inference(model, val_loader, CFG.RESULTS_DIR, save_comparisons=20, dataset_name='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9624cc9",
   "metadata": {},
   "source": [
    "---\n",
    "## 16. Inference ‚Äî Test Set Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc0d30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on test set\n",
    "run_inference(model, test_loader, CFG.RESULTS_DIR, save_comparisons=30, dataset_name='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0f4756",
   "metadata": {},
   "source": [
    "---\n",
    "## 17. Confusion Matrix & Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df3c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_confusion_matrix(model, loader, num_classes=CFG.NUM_CLASSES):\n",
    "    \"\"\"Compute full confusion matrix.\"\"\"\n",
    "    model.eval()\n",
    "    conf_matrix = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
    "    \n",
    "    for images, masks, _ in tqdm(loader, desc='Computing confusion matrix'):\n",
    "        images = images.to(device)\n",
    "        masks  = masks.to(device)\n",
    "        \n",
    "        with autocast(enabled=CFG.USE_AMP):\n",
    "            outputs = model(pixel_values=images)\n",
    "            logits_up = F.interpolate(outputs.logits, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        preds = logits_up.argmax(dim=1).cpu().numpy().flatten()\n",
    "        targets = masks.cpu().numpy().flatten()\n",
    "        \n",
    "        for t, p in zip(targets, preds):\n",
    "            if t < num_classes and p < num_classes:\n",
    "                conf_matrix[t, p] += 1\n",
    "    \n",
    "    return conf_matrix\n",
    "\n",
    "conf_matrix = compute_confusion_matrix(model, val_loader)\n",
    "\n",
    "# Normalize\n",
    "conf_norm = conf_matrix.astype(np.float32)\n",
    "row_sums = conf_norm.sum(axis=1, keepdims=True)\n",
    "conf_norm = np.divide(conf_norm, row_sums, where=row_sums!=0)\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 9))\n",
    "\n",
    "sns.heatmap(conf_norm, annot=True, fmt='.2f', cmap='Blues', xticklabels=CLASS_NAMES,\n",
    "            yticklabels=CLASS_NAMES, ax=ax1, cbar_kws={'shrink': 0.8})\n",
    "ax1.set_xlabel('Predicted', fontsize=12)\n",
    "ax1.set_ylabel('True', fontsize=12)\n",
    "ax1.set_title('Normalized Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "sns.heatmap(np.log1p(conf_matrix), annot=False, cmap='YlOrRd', xticklabels=CLASS_NAMES,\n",
    "            yticklabels=CLASS_NAMES, ax=ax2, cbar_kws={'shrink': 0.8})\n",
    "ax2.set_xlabel('Predicted', fontsize=12)\n",
    "ax2.set_ylabel('True', fontsize=12)\n",
    "ax2.set_title('Confusion Matrix (log scale)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CFG.OUTPUT_DIR, 'confusion_matrix.png'), dpi=200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa6f318",
   "metadata": {},
   "source": [
    "---\n",
    "## 18. Multi-Scale Test Time Augmentation (TTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1957c664",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def tta_inference(model, image, scales=[0.75, 1.0, 1.25], flip=True):\n",
    "    \"\"\"Multi-scale + flip test-time augmentation.\"\"\"\n",
    "    model.eval()\n",
    "    B, C, H, W = image.shape\n",
    "    final_logits = torch.zeros(B, CFG.NUM_CLASSES, H, W, device=image.device)\n",
    "    n_aug = 0\n",
    "    \n",
    "    for scale in scales:\n",
    "        sh, sw = int(H * scale), int(W * scale)\n",
    "        scaled = F.interpolate(image, size=(sh, sw), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Forward\n",
    "        with autocast(enabled=CFG.USE_AMP):\n",
    "            out = model(pixel_values=scaled).logits\n",
    "        logits = F.interpolate(out, size=(H, W), mode='bilinear', align_corners=False)\n",
    "        final_logits += logits\n",
    "        n_aug += 1\n",
    "        \n",
    "        # Flipped\n",
    "        if flip:\n",
    "            flipped = torch.flip(scaled, dims=[-1])\n",
    "            with autocast(enabled=CFG.USE_AMP):\n",
    "                out_f = model(pixel_values=flipped).logits\n",
    "            logits_f = torch.flip(F.interpolate(out_f, size=(H, W), mode='bilinear', align_corners=False), dims=[-1])\n",
    "            final_logits += logits_f\n",
    "            n_aug += 1\n",
    "    \n",
    "    return final_logits / n_aug\n",
    "\n",
    "\n",
    "# Run TTA on val set\n",
    "@torch.no_grad()\n",
    "def evaluate_with_tta(model, loader):\n",
    "    model.eval()\n",
    "    all_iou = []\n",
    "    all_dice = []\n",
    "    all_acc = []\n",
    "    \n",
    "    for images, masks, _ in tqdm(loader, desc='TTA Evaluation'):\n",
    "        images = images.to(device)\n",
    "        masks  = masks.to(device)\n",
    "        \n",
    "        logits = tta_inference(model, images, scales=[0.75, 1.0, 1.25, 1.5])\n",
    "        \n",
    "        miou, _ = compute_iou(logits, masks)\n",
    "        mdice, _ = compute_dice(logits, masks)\n",
    "        acc = compute_pixel_accuracy(logits, masks)\n",
    "        \n",
    "        all_iou.append(miou)\n",
    "        all_dice.append(mdice)\n",
    "        all_acc.append(acc)\n",
    "    \n",
    "    print(f\"\\nüî¨ TTA Results:\")\n",
    "    print(f\"  mIoU:     {np.mean(all_iou):.4f} (no TTA: {val_results['mean_iou']:.4f})\")\n",
    "    print(f\"  Dice:     {np.mean(all_dice):.4f} (no TTA: {val_results['mean_dice']:.4f})\")\n",
    "    print(f\"  Accuracy: {np.mean(all_acc):.4f} (no TTA: {val_results['pixel_acc']:.4f})\")\n",
    "\n",
    "evaluate_with_tta(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f328b76d",
   "metadata": {},
   "source": [
    "---\n",
    "## 19. Package Everything as ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead28664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_zip():\n",
    "    \"\"\"Package all outputs, results, checkpoints into a downloadable ZIP.\"\"\"\n",
    "    zip_path = '/kaggle/working/offroad_segformer_results.zip'\n",
    "    \n",
    "    print(\"üì¶ Creating results ZIP...\")\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "        # Add all outputs\n",
    "        for root_dir, dir_name in [\n",
    "            (CFG.OUTPUT_DIR, 'outputs'),\n",
    "            (CFG.RESULTS_DIR, 'results'),\n",
    "            (CFG.CHECKPOINT_DIR, 'checkpoints'),\n",
    "        ]:\n",
    "            if os.path.exists(root_dir):\n",
    "                for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "                    for filename in filenames:\n",
    "                        filepath = os.path.join(dirpath, filename)\n",
    "                        arcname = os.path.join(dir_name, os.path.relpath(filepath, root_dir))\n",
    "                        zf.write(filepath, arcname)\n",
    "        \n",
    "        # Add this notebook if available\n",
    "        nb_candidates = glob.glob('/kaggle/working/*.ipynb')\n",
    "        for nb in nb_candidates:\n",
    "            zf.write(nb, os.path.basename(nb))\n",
    "    \n",
    "    size_mb = os.path.getsize(zip_path) / (1024 * 1024)\n",
    "    print(f\"\\n‚úÖ Results ZIP created: {zip_path}\")\n",
    "    print(f\"   Size: {size_mb:.1f} MB\")\n",
    "    print(f\"\\nüì• Download from Kaggle: Output tab ‚Üí offroad_segformer_results.zip\")\n",
    "    \n",
    "    # List contents\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        names = zf.namelist()\n",
    "        dirs = set()\n",
    "        for n in names:\n",
    "            dirs.add(n.split('/')[0] if '/' in n else n)\n",
    "        print(f\"\\n   Contents ({len(names)} files):\")\n",
    "        for d in sorted(dirs):\n",
    "            count = sum(1 for n in names if n.startswith(d))\n",
    "            print(f\"     {d}/: {count} files\")\n",
    "    \n",
    "    return zip_path\n",
    "\n",
    "zip_path = create_results_zip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ec19ab",
   "metadata": {},
   "source": [
    "---\n",
    "## 20. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a637c7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"‚ïê\" * 80)\n",
    "print(\"  üåø OFF-ROAD SEGMENTATION ‚Äî FINAL SUMMARY\")\n",
    "print(\"‚ïê\" * 80)\n",
    "print(f\"\")\n",
    "print(f\"  Model:          SegFormer-B5 ({CFG.MODEL_NAME})\")\n",
    "print(f\"  Dataset:        {len(train_dataset)} train / {len(val_dataset)} val / {len(test_dataset)} test\")\n",
    "print(f\"  Image Size:     {CFG.IMG_SIZE}x{CFG.IMG_SIZE}\")\n",
    "print(f\"  Classes:        {CFG.NUM_CLASSES}\")\n",
    "print(f\"  Epochs Trained: {len(history['train_loss'])}\")\n",
    "print(f\"\")\n",
    "print(f\"  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\n",
    "print(f\"  ‚îÇ  Best Val mIoU:       {max(history['val_miou']):.4f}              ‚îÇ\")\n",
    "print(f\"  ‚îÇ  Best Val Dice:       {max(history['val_dice']):.4f}              ‚îÇ\")\n",
    "print(f\"  ‚îÇ  Best Val Accuracy:   {max(history['val_pixel_acc']):.4f}              ‚îÇ\")\n",
    "print(f\"  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n",
    "print(f\"\")\n",
    "print(f\"  üìÅ Outputs:\")\n",
    "print(f\"     {CFG.OUTPUT_DIR}/ ‚Äî plots, metrics\")\n",
    "print(f\"     {CFG.RESULTS_DIR}/ ‚Äî predictions (masks + color + comparisons)\")\n",
    "print(f\"     {CFG.CHECKPOINT_DIR}/ ‚Äî model weights (best + last + top-{CFG.KEEP_TOP_K})\")\n",
    "print(f\"     {zip_path} ‚Äî everything as ZIP\")\n",
    "print(f\"\")\n",
    "print(f\"  Features:\")\n",
    "print(f\"     ‚úÖ SegFormer-B5 (MiT encoder + MLP decoder)\")\n",
    "print(f\"     ‚úÖ Transfer Learning from ADE20K\")\n",
    "print(f\"     ‚úÖ Combined Loss (CE + Dice + Focal)\")\n",
    "print(f\"     ‚úÖ Class-weighted loss (imbalanced data)\")\n",
    "print(f\"     ‚úÖ Heavy augmentations (Albumentations)\")\n",
    "print(f\"     ‚úÖ LayerWise LR Decay (encoder vs decoder)\")\n",
    "print(f\"     ‚úÖ Cosine Warmup Scheduler\")\n",
    "print(f\"     ‚úÖ Gradient Accumulation (effective BS={CFG.BATCH_SIZE*CFG.ACCUMULATION})\")\n",
    "print(f\"     ‚úÖ Mixed Precision (AMP)\")\n",
    "print(f\"     ‚úÖ TF32 for H100\")\n",
    "print(f\"     ‚úÖ Multi-metric Early Stopping (mIoU + Loss)\")\n",
    "print(f\"     ‚úÖ Top-K Checkpoint Manager\")\n",
    "print(f\"     ‚úÖ Resumable Training (last_checkpoint.pt)\")\n",
    "print(f\"     ‚úÖ Multi-Scale TTA\")\n",
    "print(f\"     ‚úÖ Confusion Matrix\")\n",
    "print(f\"     ‚úÖ Full ZIP Export\")\n",
    "print(\"‚ïê\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
